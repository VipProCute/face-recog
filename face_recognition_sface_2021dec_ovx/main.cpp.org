/****************************************************************************
*   Generated by ACUITY 6.21.1
*   Match ovxlib 1.1.53
*
*   Neural Network application project entry file
****************************************************************************/
/*-------------------------------------------
                Includes
-------------------------------------------*/
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#ifdef __linux__
#include <time.h>
#include <inttypes.h>
#elif defined(_WIN32)
#include <windows.h>
#endif

#define _BASETSD_H

#include "vsi_nn_pub.h"

#include "vnn_global.h"
#include "vnn_pre_process.h"
#include "vnn_post_process.h"

extern "C" {
#include "vnn_.h"
}

/*-------------------------------------------
        Macros and Variables
-------------------------------------------*/
#ifdef __linux__
#define VSI_UINT64_SPECIFIER PRIu64
#elif defined(_WIN32)
#define VSI_UINT64_SPECIFIER "I64u"
#endif

/*-------------------------------------------
                  Functions
-------------------------------------------*/
static void vnn_ReleaseNeuralNetwork
    (
    vsi_nn_graph_t *graph
    )
{
    vnn_Release( graph, TRUE );
    if (vnn_UseImagePreprocessNode())
    {
        vnn_ReleaseBufferImage();
    }
}

static vsi_status vnn_PostProcessNeuralNetwork
    (
    vsi_nn_graph_t *graph
    )
{
    return vnn_PostProcess( graph );
}

#define BILLION                                 1000000000
static uint64_t get_perf_count()
{
#if defined(__linux__) || defined(__ANDROID__) || defined(__QNX__) || defined(__CYGWIN__)
    struct timespec ts;

    clock_gettime(CLOCK_MONOTONIC, &ts);

    return (uint64_t)((uint64_t)ts.tv_nsec + (uint64_t)ts.tv_sec * BILLION);
#elif defined(_WIN32) || defined(UNDER_CE)
    LARGE_INTEGER freq;
    LARGE_INTEGER ln;

    QueryPerformanceFrequency(&freq);
    QueryPerformanceCounter(&ln);

    return (uint64_t)(ln.QuadPart * BILLION / freq.QuadPart);
#endif
}

static vsi_status vnn_VerifyGraph
    (
    vsi_nn_graph_t *graph
    )
{
    vsi_status status = VSI_FAILURE;
    uint64_t tmsStart, tmsEnd, msVal, usVal;

    /* Verify graph */
    printf("Verify...\n");
    tmsStart = get_perf_count();
    status = vsi_nn_VerifyGraph( graph );
    TEST_CHECK_STATUS(status, final);
    tmsEnd = get_perf_count();
    msVal = (tmsEnd - tmsStart)/1000000;
    usVal = (tmsEnd - tmsStart)/1000;
    printf("Verify Graph: %"VSI_UINT64_SPECIFIER"ms or %"VSI_UINT64_SPECIFIER"us\n", msVal, usVal);

final:
    return status;
}

static vsi_status vnn_ProcessGraph
    (
    vsi_nn_graph_t *graph
    )
{
    vsi_status status = VSI_FAILURE;
    int32_t i,loop;
    char *loop_s;
    uint64_t tmsStart, tmsEnd, sigStart, sigEnd;
    float msVal, usVal;

    status = VSI_FAILURE;
    loop = 1; /* default loop time is 1 */
    loop_s = getenv("VNN_LOOP_TIME");
    if(loop_s)
    {
        loop = atoi(loop_s);
    }

    /* Run graph */
    tmsStart = get_perf_count();
    printf("Start run graph [%d] times...\n", loop);
    for(i = 0; i < loop; i++)
    {
        sigStart = get_perf_count();
#ifdef VNN_APP_ASYNC_RUN
        status = vsi_nn_AsyncRunGraph( graph );
        if(status != VSI_SUCCESS)
        {
            printf("Async Run graph the %d time fail\n", i);
        }
        TEST_CHECK_STATUS( status, final );

        //do something here...

        status = vsi_nn_AsyncRunWait( graph );
        if(status != VSI_SUCCESS)
        {
            printf("Wait graph the %d time fail\n", i);
        }
#else
        status = vsi_nn_RunGraph( graph );
        if(status != VSI_SUCCESS)
        {
            printf("Run graph the %d time fail\n", i);
        }
#endif
        TEST_CHECK_STATUS( status, final );

        sigEnd = get_perf_count();
        msVal = (sigEnd - sigStart)/(float)1000000;
        usVal = (sigEnd - sigStart)/(float)1000;
        printf("Run the %u time: %.2fms or %.2fus\n", (i + 1), msVal, usVal);
    }
    tmsEnd = get_perf_count();
    msVal = (tmsEnd - tmsStart)/(float)1000000;
    usVal = (tmsEnd - tmsStart)/(float)1000;
    printf("vxProcessGraph execution time:\n");
    printf("Total   %.2fms or %.2fus\n", msVal, usVal);
    printf("Average %.2fms or %.2fus\n", ((float)usVal)/1000/loop, ((float)usVal)/loop);

final:
    return status;
}

static vsi_status vnn_PreProcessNeuralNetwork
    (
    vsi_nn_graph_t *graph,
    int argc,
    char **argv
    )
{
    /*
     * argv0:   execute file
     * argv1:   data file
     * argv2~n: inputs n file
     */
    const char **inputs = (const char **)argv + 2;
    uint32_t input_num = argc - 2;

    return vnn_PreProcess( graph, inputs, input_num );
}

static vsi_nn_graph_t *vnn_CreateNeuralNetwork
    (
    const char *data_file_name
    )
{
    vsi_nn_graph_t *graph = NULL;
    uint64_t tmsStart, tmsEnd, msVal, usVal;

    tmsStart = get_perf_count();
    graph = vnn_Create( data_file_name, NULL,
                      vnn_GetPreProcessMap(), vnn_GetPreProcessMapCount(),
                      vnn_GetPostProcessMap(), vnn_GetPostProcessMapCount() );
    TEST_CHECK_PTR(graph, final);

    tmsEnd = get_perf_count();
    msVal = (tmsEnd - tmsStart)/1000000;
    usVal = (tmsEnd - tmsStart)/1000;
    printf("Create Neural Network: %"VSI_UINT64_SPECIFIER"ms or %"VSI_UINT64_SPECIFIER"us\n", msVal, usVal);

final:
    return graph;
}

/*-------------------------------------------
                  Main Functions
-------------------------------------------*/
int main
    (
    int argc,
    char **argv
    )
{
    vsi_status status = VSI_FAILURE;
    vsi_nn_graph_t *graph;
    const char *data_name = NULL;

    if(argc < 3)
    {
        printf("Usage: %s data_file inputs...\n", argv[0]);
        return -1;
    }

    data_name = (const char *)argv[1];

    /* Create the neural network */
    graph = vnn_CreateNeuralNetwork( data_name );
    TEST_CHECK_PTR( graph, final );

    /* Verify graph */
    status = vnn_VerifyGraph( graph );
    TEST_CHECK_STATUS( status, final);

    /* Pre process the image data */
    status = vnn_PreProcessNeuralNetwork( graph, argc, argv );
    TEST_CHECK_STATUS( status, final );



    /* Process graph */
    status = vnn_ProcessGraph( graph );
    TEST_CHECK_STATUS( status, final );

    if(VNN_APP_DEBUG)
    {
        /* Dump all node outputs */
        vsi_nn_DumpGraphNodeOutputs(graph, "./network_dump", NULL, 0, TRUE, 0);
    }

    /* Post process output data */
    status = vnn_PostProcessNeuralNetwork( graph );
    TEST_CHECK_STATUS( status, final );

final:
    vnn_ReleaseNeuralNetwork( graph );
    fflush(stdout);
    fflush(stderr);
    return status;
}

#if 0
// YuNet on CPU
const std::map<std::string, int> str2backend{
    {"opencv", cv::dnn::DNN_BACKEND_OPENCV}, {"cuda", cv::dnn::DNN_BACKEND_CUDA},
    {"timvx",  cv::dnn::DNN_BACKEND_TIMVX},  {"cann", cv::dnn::DNN_BACKEND_CANN}
};
const std::map<std::string, int> str2target{
    {"cpu", cv::dnn::DNN_TARGET_CPU}, {"cuda", cv::dnn::DNN_TARGET_CUDA},
    {"npu", cv::dnn::DNN_TARGET_NPU}, {"cuda_fp16", cv::dnn::DNN_TARGET_CUDA_FP16}
};

class YuNet
{
public:
    YuNet(const std::string& model_path,
          const cv::Size& input_size = cv::Size(320, 320),
          float conf_threshold = 0.6f,
          float nms_threshold = 0.3f,
          int top_k = 5000,
          int backend_id = 0,
          int target_id = 0)
        : model_path_(model_path), input_size_(input_size),
          conf_threshold_(conf_threshold), nms_threshold_(nms_threshold),
          top_k_(top_k), backend_id_(backend_id), target_id_(target_id)
    {
        model = cv::FaceDetectorYN::create(model_path_, "", input_size_, conf_threshold_, nms_threshold_, top_k_, backend_id_, target_id_);
    }

    /* Overwrite the input size when creating the model. Size format: [Width, Height].
    */
    void setInputSize(const cv::Size& input_size)
    {
        input_size_ = input_size;
        model->setInputSize(input_size_);
    }

    cv::Mat infer(const cv::Mat image)
    {
        cv::Mat res;
        model->detect(image, res);
        return res;
    }

private:
    cv::Ptr<cv::FaceDetectorYN> model;

    std::string model_path_;
    cv::Size input_size_;
    float conf_threshold_;
    float nms_threshold_;
    int top_k_;
    int backend_id_;
    int target_id_;
};

cv::Mat visualize(const cv::Mat& image, const cv::Mat& faces, float fps = -1.f)
{
    static cv::Scalar box_color{0, 255, 0};
    static std::vector<cv::Scalar> landmark_color{
        cv::Scalar(255,   0,   0), // right eye
        cv::Scalar(  0,   0, 255), // left eye
        cv::Scalar(  0, 255,   0), // nose tip
        cv::Scalar(255,   0, 255), // right mouth corner
        cv::Scalar(  0, 255, 255)  // left mouth corner
    };
    static cv::Scalar text_color{0, 255, 0};

    auto output_image = image.clone();

    if (fps >= 0)
    {
        cv::putText(output_image, cv::format("FPS: %.2f", fps), cv::Point(0, 15), cv::FONT_HERSHEY_SIMPLEX, 0.5, text_color, 2);
    }

    for (int i = 0; i < faces.rows; ++i)
    {
        // Draw bounding boxes
        int x1 = static_cast<int>(faces.at<float>(i, 0));
        int y1 = static_cast<int>(faces.at<float>(i, 1));
        int w = static_cast<int>(faces.at<float>(i, 2));
        int h = static_cast<int>(faces.at<float>(i, 3));
        cv::rectangle(output_image, cv::Rect(x1, y1, w, h), box_color, 2);

        // Confidence as text
        float conf = faces.at<float>(i, 14);
        cv::putText(output_image, cv::format("%.4f", conf), cv::Point(x1, y1+12), cv::FONT_HERSHEY_DUPLEX, 0.5, text_color);
#if 0
        // Draw landmarks
        for (int j = 0; j < landmark_color.size(); ++j)
        {
            int x = static_cast<int>(faces.at<float>(i, 2*j+4)), y = static_cast<int>(faces.at<float>(i, 2*j+5));
            cv::circle(output_image, cv::Point(x, y), 2, landmark_color[j], 2);
        }
#endif
    }
    return output_image;
}
#endif

#if 0
// for testing

int main
    (
    int argc,
    char **argv
    )
{
    vsi_status status = VSI_FAILURE;
    vsi_nn_graph_t *graph;
    const char *data_name = NULL;

    if(argc < 3)
    {
        printf("Usage: %s data_file inputs...\n", argv[0]);
        return -1;
    }

    data_name = (const char *)argv[1];
    std::string image_path = argv[2];

    cv::Mat orig_image = cv::imread(image_path);
    cv::cvtColor(orig_image, orig_image, cv::COLOR_BGR2RGB);

# if 1  // use yunet model run on CPU
        cv::CommandLineParser parser(argc, argv,
            "{help  h           |                                   | Print this message}"
            "{input i           |                                   | Set input to a certain image, omit if using camera}"
            "{model m           | face_detection_yunet_2023mar.onnx  | Set path to the model}"
            "{backend b         | opencv                            | Set DNN backend}"
            "{target t          | cpu                               | Set DNN target}"
            "{save s            | false                             | Whether to save result image or not}"
            "{vis v             | false                             | Whether to visualize result image or not}"
            /* model params below*/
            "{conf_threshold    | 0.9                               | Set the minimum confidence for the model to identify a face. Filter out faces of conf < conf_threshold}"
            "{nms_threshold     | 0.3                               | Set the threshold to suppress overlapped boxes. Suppress boxes if IoU(box1, box2) >= nms_threshold, the one of higher score is kept.}"
            "{top_k             | 5000                              | Keep top_k bounding boxes before NMS. Set a lower value may help speed up postprocessing.}"
        );
        std::string detector_model_path = "face_detection_yunet_2023mar.onnx";
        std::string backend = "opencv";
        std::string target = "cpu";
        // std::string backend = "timvx";
        // std::string target = "npu";
        bool save_flag = parser.get<bool>("save");
        bool vis_flag = parser.get<bool>("vis");

        // model params
        float conf_threshold = parser.get<float>("conf_threshold");
        float nms_threshold = parser.get<float>("nms_threshold");
        int top_k = parser.get<int>("top_k");
        const int backend_id = str2backend.at(backend);
        const int target_id = str2target.at(target);

        uint64_t tmsStart, tmsEnd, msVal, usVal;

        std::cout << __FUNCTION__ << "():" << __LINE__ << std::endl;
        std::cout << "conf_threshold: " << conf_threshold << std::endl;
        std::cout << "nms_threshold: " << nms_threshold << std::endl;
        std::cout << "top_k: " << top_k << std::endl;
        std::cout << "backend_id: " << backend_id << std::endl;
        std::cout << "target_id: " << target_id << std::endl;
        

        // std::string input_path_1 = "vmx_1_640_640.jpg";
        // Instantiate YuNet
        YuNet face_detector(detector_model_path, cv::Size(320, 320), conf_threshold, nms_threshold, top_k, backend_id, target_id);
        // auto face_detector = YuNet(
        //     detector_model_path, cv::Size(320, 320), conf_threshold, nms_threshold, top_k, backend_id, target_id);


        cv::Mat image;
        // auto image_1 = cv::imread(input_path_1);
        image = orig_image.clone();
        // cv::resize(image, image, cv::Size(640, 640));

        tmsStart = get_perf_count();
        // Inference
        face_detector.setInputSize(image.size());
        // face_detector.setTopK(1);
        cv::Mat faces = face_detector.infer(image);

        tmsEnd = get_perf_count();
        msVal = (tmsEnd - tmsStart) / 1000000;
        usVal = (tmsEnd - tmsStart) / 1000;
        printf("face_detection_result.face_count: %d in %ld ms or %ld us\n", faces.rows, msVal, usVal);
        std::cout << cv::format("msVal: %ld ms or %ld us\n", msVal, usVal);

        cv::Mat face_image = faces.row(0);
        cv::Mat target_aligned;
        alignCrop(orig_image, face_image, target_aligned);
#if 1
            char saved_face[128];
            // sprintf(saved_face, "detected_face_%d.jpg", i);
            sprintf(saved_face, "target_aligned.jpg");
            if (!target_aligned.empty()) {
                cv::imwrite(saved_face, target_aligned);
            }
#endif
#if 0
        cv::Mat detected_faces[8];

        // Print faces
        std::cout << cv::format("%d faces detected:\n", faces.rows);
        for (int i = 0; i < faces.rows; ++i)
        {
            int x1 = static_cast<int>(faces.at<float>(i, 0));
            int y1 = static_cast<int>(faces.at<float>(i, 1));
            int w = static_cast<int>(faces.at<float>(i, 2));
            int h = static_cast<int>(faces.at<float>(i, 3));
            float conf = faces.at<float>(i, 14);
            std::cout << cv::format("%d: x1=%d, y1=%d, w=%d, h=%d, conf=%.4f\n", i, x1, y1, w, h, conf);
            cv::Rect b_box(x1, y1, w, h);
            detected_faces[i] = image(b_box);
        }
#endif

#if 0
        auto res_image = visualize(image, faces);
        // if (save_flag)
        {
            std::cout << "Results are saved to result.jpg\n";
            cv::imwrite("result.jpg", res_image);
        }
#endif


#endif

    status = extractFeatures(target_aligned);

    return status;
}
#endif
